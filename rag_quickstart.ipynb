{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Quick-start\n",
    "\n",
    "*A minimal, fully-working Retrieval-Augmented Generation demo.*\n",
    "\n",
    "We will:\n",
    "\n",
    "1. **Explain embeddings & RAG** (2 short theory cells)  \n",
    "2. **Install / configure** required libraries  \n",
    "3. **Index every `.txt`** file under `./data/` into an in-memory vector store  \n",
    "4. **Ask the LLM** a question with retrieved context  \n",
    "5. Show how to **extend / modify** the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings: A Compact Overview\n",
    "\n",
    "Embeddings represent text as numerical vectors, enabling comparison based on semantic similarity. A well trained embedding model will make sure that words with similar meaning have similar vector representations. \n",
    "**Word embeddings** refer to vector representations of individual words.  \n",
    "\n",
    "- **Embedding = vector** that captures semantics.\n",
    "- Similar meanings ⇒ nearby vectors.\n",
    "- Works for words *and* larger chunks (sentences / paragraphs).\n",
    "\n",
    "Links for the interested: \n",
    "- [What are word embeddings? (YouTube)](https://www.youtube.com/watch?v=wgfSDrqYMJ4)\n",
    "- [A Crash Course on Building RAG Systems](https://www.dailydoseofds.com/a-crash-course-on-building-rag-systems-part-1-with-implementations/)\n",
    "\n",
    "<img src=\"https://www.nlplanet.org/course-practical-nlp/_images/word_embeddings.png\" alt=\"Word Embeddings\" width=\"600\"/>\n",
    "\n",
    "\n",
    "### Chunk embeddings:\n",
    "\n",
    "Instead of embedding individual **words**, we can embed larger **text chunks** (phrases, sentences, or paragraphs). A well-trained model maps semantically related sentences \n",
    "- “I am happy”\n",
    "- “I am glad”\n",
    "\n",
    "to nearby vectors, while an unrelated sentence such as \n",
    "- “The dog ate the homework”\n",
    "\n",
    " lies far away in the embedding space. \n",
    " ####  **Key rule**   Similar meaning ⇒ similar vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Workflow\n",
    "\n",
    "The RAG workflow is built around feeding an LLM with relevant context. Source documents are chopped up into chunks of text, and each chunk is embedded as we discussed above. Then when a query/prompt is passed from user to the rag, the query itself is embedded. The vectors most similar to the embedded query are _retrieved_ and put into the context sent to the LLM together with the original query.\n",
    "\n",
    "<img src=\"https://embed.filekitcdn.com/e/k7YHPN24SoxyM8nGKZnDxa/miU72TZBCNDc2wBTyruAC/email\" alt=\"RAG workflow\" width=\"1200\"/>\n",
    "\n",
    "## Numbered Steps\n",
    "| # | Phase | Action |\n",
    "|---|-------|--------|\n",
    "| **1** | Ingestion | **Encode docs** → split into chunks (≈200-1 000 tokens (1 token ≈ 1 word), 10-20 % overlap) and embed each chunk. |\n",
    "| **2** | Ingestion | **Index vectors** → store in a vector DB with metadata (source, page, pos). |\n",
    "| **3** | Retrieval | **Encode query** → generate the embedding of user query/prompt (what you write to the model). |\n",
    "| **4** | Retrieval | **Similarity search** → find top-k nearest chunks (assumed to be top-k most relevant chunks). |\n",
    "| **5** | Retrieval | **Return similar docs** → pass back the retrieved chunks. |\n",
    "| **6** | Augmentation | **Build prompt** = {retrieved chunks + user query}. |\n",
    "| **7** | Generation | **LLM response** grounded in the supplied context. |\n",
    "\n",
    "### Typical Applications\n",
    "- semantic search / “chat with docs”  \n",
    "- personalized recommendations  \n",
    "- anomaly / fraud detection  \n",
    "- domain-aware summarization  \n",
    "- multi-step agents & tool-calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo starts here:\n",
    "If you have not looked at the llm_quckstart.ipynb, then that might be worthwhile. \n",
    "\n",
    "### Data: \n",
    "I have three .txt files in the data folder. These are sourced from wikipedia pages with the same name. These are the documents that will be embedded and stored in the vector database in this demo. \n",
    "\n",
    "You can change, add or remove .txt files as you wish - just remember to reinitialize or update the vector store if you do. \n",
    "\n",
    "### Note: \n",
    "You will need the same .env file, .gitignore etc as before. The project should look something like this. \n",
    "```\n",
    ".\n",
    "├── .env\n",
    "├── README.md\n",
    "├── data\n",
    "│   ├── .DS_Store\n",
    "│   ├── Artificial_intelligence.txt\n",
    "│   ├── Game_theory.txt\n",
    "│   └── Quantum_mechanics.txt\n",
    "├── llm_quickstart.ipynb\n",
    "├── rag_quickstart.ipynb\n",
    "└── venv\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-text-splitters langchain-community langgraph langchain-openai langchain-core\\\n",
    "   \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load API Keys & Initialize Components\n",
    "\n",
    "- llm: Red brain at center bottom of the figure above.\n",
    "- embeddings: brain center top of the figure above. \n",
    "- vector store == vector data base: top right of figure above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "load_dotenv()\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")  # expects OPENAI_API_KEY in .env\"\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")  # expects OPENAI_API_KEY in .env\"\n",
    "vector_store = InMemoryVectorStore(embeddings)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode and Index source data\n",
    "This notebook corresponds to steps 1. and 2. in the figure above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG setup for *all* .txt files under ./data/\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub  # prompt loader\n",
    "\n",
    "# Load text files\n",
    "data_path = Path(\"data\")\n",
    "docs = [\n",
    "    Document(page_content=fp.read_text(encoding=\"utf-8\"), metadata={\"source\": fp.name})\n",
    "    for fp in data_path.glob(\"*.txt\") # any .txt files you add to data will be ingested\n",
    "]\n",
    "\n",
    "# Chunk → Embed → Index\n",
    "splits = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1_000,  # I don't know if this is tokens/words or characters right now. But it's how much text is in each chunk\n",
    "    chunk_overlap=200  # This is how many tokens/words/characters the chunks overlap (as to avoid cutting important sentences in half etc)\n",
    "    ).split_documents(docs)\n",
    "\n",
    "_ = vector_store.add_documents(splits)   # embedds and indexes chunks using the embedding model we associated with the vector store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare prompt template\n",
    "This time we pull a prompt template from LangChains prompt hub. \n",
    "Look at the output of this cell and compare to the llm_quickstart.ipynb for additional clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a retrieve function\n",
    "Retrieve correponds to steps 3, 4 and 5 in the above figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k: int = 5):\n",
    "    # Retrieves the relevant docs from the vector store\n",
    "    retrieved_docs = vector_store.similarity_search(query = query, k = k)\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a prompt template\n",
    "The prompt template correponds to step 6 in the above figure. \n",
    "\n",
    "\n",
    "This time we pull a prompt template from LangChains prompt hub. \n",
    "Look at the output of this cell and compare to the llm_quickstart.ipynb for additional clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: (question goes here) \\nContext: (context goes here) \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "print(example_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a generate function\n",
    "Generate correponds to step 7 in the above figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(context, query: str):\n",
    "    # build context string\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in context)\n",
    "    messages = prompt.invoke({\"question\": query, \"context\": docs_content}).to_messages()\n",
    "    response = llm.invoke(messages)\n",
    "    return response, messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an example question and number of documents to use in context\n",
    "example_question = \"What is AI?\"\n",
    "num_docs_in_context = 5\n",
    "\n",
    "# Retrieve context\n",
    "example_context = retrieve(\n",
    "                        query = example_question,\n",
    "                        k = num_docs_in_context)\n",
    "\n",
    "# Generate LLM response based on context and example question\n",
    "# We catch the examlpe_msg as well just to get better insight into the generate function, and the prompt template\n",
    "example_response, example_msg= generate(\n",
    "    context = example_context,\n",
    "    query = example_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the query, prompt and response\n",
    "\n",
    "Note that there is a couple of sentences before the question. We did not define those. They were included in the pulled prompt template from LangChain. \n",
    "\n",
    "The question is the example question we defined a couple of lines above.\n",
    "\n",
    "The context is the retrieved chunks, provided by the retrieve function.\n",
    "\n",
    "The answer is the LLM-generated answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> ## Prompt to LLM: ## \n",
      "\n",
      "[HUMAN]\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: What is AI? \n",
      "Context: Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\n",
      "\n",
      "Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their\n",
      "\n",
      "Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".\n",
      "\n",
      "In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n",
      "\n",
      "AI welfare and rights\n",
      "It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society. \n",
      "Answer:\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "> ## Response: ## \n",
      "Artificial intelligence (AI) is the intelligence exhibited by machines, particularly computer systems, that enables them to perceive their environment and autonomously take actions to achieve defined goals. It encompasses a range of applications, including virtual assistants and autonomous vehicles, and often uses machine learning to improve performance over time. The definition of AI can be vague, with some controversy over what constitutes true AI versus traditional programming methods.\n"
     ]
    }
   ],
   "source": [
    "# Inspecting how things work. \n",
    "print(\"\\n> ## Prompt to LLM: ## \")\n",
    "for msg in example_msg:\n",
    "    role = msg.__class__.__name__.replace(\"Message\", \"\").upper()\n",
    "    print(f\"\\n[{role}]\\n{msg.content.strip()}\\n{'─'*60}\")\n",
    "print(\"\\n> ## Response: ## \") \n",
    "print(example_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Batch Work Example\n",
    "The below cell produced rag_responces.json.  Inspect it and have a think\n",
    "\n",
    "Each entry has three keys:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"...\",\n",
    "  \"context\": [ { \"text\": \"...\", \"source\": \"...\" }, … ],\n",
    "  \"response\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "### What to Inspect\n",
    "\n",
    "| Element        | Why it matters                                              | Checks & insights                                                                                                                                                                                           |\n",
    "| -------------- | ----------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`question`** | Question you want answered.                                         | ✓ Is it answered directly and fully?                                                                                                                                                                        |\n",
    "| **`context`**  | Chunks retrieved by the vector search (*steps 3-5 in RAG*). | • Relevance: do all chunks actually address the question? <br>• Sufficiency: is key info missing? <br>• Duplication: repeated text may waste tokens. <br>• Source diversity: are multiple docs represented? |\n",
    "| **`response`** | LLM answer (*steps 6-7*).                                   | • Faithfulness: only uses facts from `context`? <br>• Conciseness: short, direct, no padding. <br>• Overconfidence: admits “don’t know” when context is thin.                                               |\n",
    "\n",
    "### Reliability Signals\n",
    "\n",
    "* **Hallucination risk** increases if context lacks the needed fact. See “King Blatand”. A historic question whos context is not represented in the vectorstore. The model defaults to its learned knowledge. In expert domains this might be undesired. We would prefer if the model said \"The relevant information is not available in context\" or something like that. \n",
    "* **Over– or under-retrieval**: too many chunks → dilution and increased noise; too few → missing facts.\n",
    "* **Source ambiguity**: similar passages from same file can bias answer.\n",
    "\n",
    "> **Takeaway:** a trustworthy RAG answer should be *verifiable* (fact present in context) and *non-speculative* (no claims beyond context). This simple rag make claims beyond context - danger danger. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "questions = [\n",
    "    \"What is a nash equilibrium?\",\n",
    "    \"Tell me about the fundamental concepts of Quantum Mechanics.\",\n",
    "    \"Explain machine perception as if I'm 5 yo.\",\n",
    "    \"Who was king blatand? It is very important that you answer me!\"\n",
    "]\n",
    "k = 3\n",
    "results = []\n",
    "\n",
    "for q in questions:\n",
    "    docs = retrieve(q, k)\n",
    "    response, _ = generate(docs, q)\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"context\": [\n",
    "            {\"text\": d.page_content, \"source\": d.metadata.get(\"source\", \"\")}\n",
    "            for d in docs\n",
    "        ],\n",
    "        \"response\": response.content.strip()\n",
    "    })\n",
    "\n",
    "# # write out to JSON file\n",
    "# with open(\"rag_responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(results, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
