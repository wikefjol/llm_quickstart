{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d73f08",
   "metadata": {},
   "source": [
    "# LangChainÂ LLM Quickâ€‘start\n",
    "Based on : https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "For RAGS : https://python.langchain.com/docs/tutorials/rag/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b6958",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup (one-time)\n",
    "1. **Create & activate virtual-env**  \n",
    "   ```bash\n",
    "   python -m venv venv\n",
    "   source venv/bin/activate   # Windows: .\\venv\\Scripts\\activate\n",
    "   ```\n",
    "2. **Upgrade pip & install deps**  \n",
    "   ```bash\n",
    "   pip install -U pip \\\n",
    "      langchain-text-splitters langchain-community langgraph python-dotenv \"langchain[openai]\"\n",
    "   ```\n",
    "3. **Secrets** â€“ create `.env`  \n",
    "   ```env\n",
    "   OPENAI_API_KEY=sk-...\n",
    "   LANGSMITH_API_KEY=lsv2_...\n",
    "   ```\n",
    "4. **Add `.gitignore` that looks like this**  \n",
    "   ```gitignore\n",
    "   .env\n",
    "   venv/\n",
    "   __pycache__/\n",
    "   ```\n",
    "5. **Project tree**  \n",
    "   ```text\n",
    "   .\n",
    "   â”œâ”€â”€ .env\n",
    "   â”œâ”€â”€ .gitignore\n",
    "   â”œâ”€â”€ llm_quickstart.ipynb\n",
    "   â””â”€â”€ venv/\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7212ad9",
   "metadata": {},
   "source": [
    "## Install/run once\n",
    "Restart the kernel after installation if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294ff50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-text-splitters langchain-community langgraph python-dotenv \\\n",
    "  \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bf061",
   "metadata": {},
   "source": [
    "## Configure API keys (`.env`)\n",
    "```env\n",
    "OPENAI_API_KEY=sk-...\n",
    "LANGSMITH_API_KEY=lsv2_...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82f9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "load_dotenv()\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d756b4",
   "metadata": {},
   "source": [
    "## Oneâ€‘off call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee3f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ripe banana is typically yellow. However, bananas can also be green when unripe and brown or black when overripe.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Which colour is a banana?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c1895",
   "metadata": {},
   "source": [
    "## System context & streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a33a42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'IA Ã¨ molto cool!"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following into Italian\"),\n",
    "    HumanMessage(\"AI is very cool!\")\n",
    "]\n",
    "\n",
    "for token in llm.stream(messages):\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481ad9e",
   "metadata": {},
   "source": [
    "## Prompt templates for batch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a4dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le maire a commis un crime !\n",
      "Â¡Chalmers es la mejor universidad del mundo!\n",
      "Sex laxar i en laxburk!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Translate the following from English into {language}\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"French\", \"The mayor has committed a crime!\"),\n",
    "    (\"Spanish\", \"Chalmers is the best university in the world!\"),\n",
    "    (\"Swedish\", \"Six salmons in a salmon tin!\")\n",
    "]\n",
    "\n",
    "for language, text in data:\n",
    "    prompt = template.invoke({\"language\": language, \"text\": text})\n",
    "    print(llm.invoke(prompt).content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
